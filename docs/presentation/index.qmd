---
title: Fast and Scalable Mining of Time Series Motifs with Probabilistic Guarantees
author: Matteo Ceccarello
email: matteo.ceccarello@unipd.it
institute: University of Padova
jointwork: Johann Gamper (U. Bolzano)
format: 
  revealjs:
    embed-resources: true
    self-contained-math: true
    css: styles.css
    slide-number: true
    template-partials:
      - title-slide.html
revealjs-plugins:
  - pointer
---

```{python}
import polars as pl
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import numpy as np
from numpy.lib.stride_tricks import sliding_window_view
from utils import *

w_insect = 400
insect_labels = {
    6974: 'a',
    8490: 'b',
    886: 'c',
    2808: 'd',
    592: 'e',
    2085: 'f',
    50: 'g',
    4050: 'h'
}
insect = np.loadtxt("insect15.txt")

w = 640
i, j = 643, 8724
motif_idxs = np.array([i, j], dtype=np.int64)

np.random.seed(1234)
steamgen = pl.read_csv("steamgen.csv")
steam = steamgen.select("steam flow").to_numpy()[:, 0]
sample_idxs = np.random.choice(
    np.arange(steam.shape[0]-w, dtype=np.int64), 20)
p_idxs = np.concatenate([motif_idxs, sample_idxs])

windows = sliding_window_view(steam, w)
swindows = windows[p_idxs, :]
embedder = PCA(n_components=2)
prj = embedder.fit_transform(swindows)
np.random.seed(1234)
prj = prj + np.random.normal(scale=8, size=prj.shape)
prj = MinMaxScaler(feature_range=(-0.5, 0.5)).fit_transform(prj)
```

## Outline {visibility="hidden"}

1. Problem definition
2. State of the art
3. Locality Sensitive Hashing
4. Our algorithm
5. Experimental results

## Timeseries and subsequences {.smaller}

::::{.r-stack}
:::{}
```{python}
plot_ts(insect, axis=True)
```
:::

:::{.fragment fragment-index=1}
```{python}
plot_ts(insect, w=w_insect, highlight=[200], axis=True)
```
:::

:::{.fragment fragment-index=2}
```{python}
plot_ts(insect, w=w_insect, highlight=[200, 6974], axis=True)
```
:::

:::{.fragment fragment-index=3}
```{python}
plot_ts(insect, w=w_insect, highlight=[200, 6974, 8490], axis=True)
```
:::
::::

:::{.fragment fragment-index=1}
Fix a subsequence length $w$
:::

::::{.columns}
:::{.column .fragment fragment-index=2}
```{python}
plot_eucl(insect, w=w_insect, i=6974, j=200, ci="steelblue", cj="forestgreen")
```
:::

:::{.column .fragment fragment-index=3}
```{python}
plot_eucl(insect, w=w_insect, i=6974, j=8490, ci="steelblue", cj="orange")
```
:::
::::

. . .

- The Euclidean distance measures the _similarity_ between subsequences
- Subsequences can be seen as points in a $w$ dimensional space

## {.smaller}

<center>
```{python}
plot_ts(insect, w=w_insect, highlight=[6974, 8490], axis=True, colors=["steelblue", "orange"])
```
</center>

This timeseries is from an experiment recording the behavior of a _Circulifer tenellus_ insect.
In particular, the subsequences below are associated with a particular behavior while feeding.

<center>
```{python}
plot_eucl(insect, w=w_insect, i=6974, j=8490, ci="steelblue", cj="orange")
```
</center>


:::{.aside}
Example taken from
Mueen et al. _Exact Discovery of Time Series Motifs._ **SDM** 2009
:::

## Top-$k$ motifs

:::::{.columns}

::::{.column width="70%"}

```{python}
pairs_occs = [
  (6974, 8490),
  (886, 2808),
  (592, 2085)
  #(50, 4050)
]
plot_in_context(
  insect[:10000],
    pairs_occs,
    w_insect,
    labels = insect_labels,
    labelpos = {
        886: insect.min(),
        2808: insect.min(),
    })
```

:::{#def-problem}
Given a time series $T$ and a length
$w$, the top-$k$ motifs are the $k$ pairs of subsequences of $T$ of length $w$
with smallest distance, such that no two subsequences in any pair
overlap with each other.
:::

::::

::::{.column width="30%"}

```{python}
plot_catalog(insect, pairs_occs, w_insect, spacing=7, labels=insect_labels)
```
::::

:::::

## Our contribution

- We propose `Attimo`, a randomized algorithm for discovering the _exact_ top-$k$ motifs
- `Attimo` auto tunes its parameters to the data distribution
- `Attimo` allows to control the success probability, i.e. its recall
- `Attimo` can handle time series of one billion values in just half an hour

## State of the art: Matrix Profile {.smaller}

::: {.callout .fragment}
### Algorithm
- For each subsequence, find the nearest neighbor
- Find the subsequence with the closest nearest neighbor
:::

::: {.callout .fragment}
### Pros
- Leverage the fact that consecutive subsequences share a lot of structure
- Parallelize with many GPUs
- Finds motifs out of 1 billion long time series in one day, using 5 GPUs 
:::

::: {.callout .fragment}
### Cons
- it's a $\Omega(n^2)$ algorithm, for timeseries of length $n$
:::


::: aside
Zimmerman et al.
_Scaling Time Series Motif Discovery with GPUs to Break a Quintillion Pairwise Comparisons a Day and Beyond._
SoCC19
:::

##

### Goal

- Find the top motifs without computing all-pairs-distances

:::{.fragment}

### Challenges

- Subsequences can be seen as vectors
- These vectors are high dimensional
- Curse of dimensionality: indices such as R-trees degrade to linear scans
:::

## Locality Sensitive Hashing {.smaller}

Subsequences of length $w$ are points in $R^w$,
<br/>with Euclidean distance.

:::{.absolute top="20%"}
```{python}
plot_projection_circle(prj)
```
:::

:::{.fragment .absolute top="20%" fragment-index=1}
```{python}
plot_hashes(prj, k=1, r=0.25, seed=1234)
plt.savefig("imgs/steam-hashes-k1-l1.png", dpi=300)
```
:::

:::{.fragment .fade-in-then-out .absolute top="20%" left="54%" fragment-index=1}

- choose $r \in \mathbb{R}^+$
- sample $\vec{a} \sim \mathcal{N}(0,1)^w$, $b \sim \mathcal{U}(0,r)$

$$
h(\vec{x}) = \left\lfloor\frac{\vec{a} \cdot \vec{x} + b}{r}\right\rfloor
$$

The key point is that we only compute the distance 
of
subsequences falling <br/>into the same bucket.
:::

:::{.fragment .absolute top="25%" left="54%" fragment-index=2}
```{python}
plot_cp(w, 1)
plt.savefig("imgs/steam-cp-k1-l1.png", dpi=300)
```
:::

## {.smaller}

:::{.absolute top="20%" fragment-index=1}
```{python}
plot_hashes(prj, k=2, r=0.25, seed=1234)
plt.savefig("imgs/steam-hashes-k2-l1.png", dpi=300)
```
:::

:::{.absolute top=10 left="54%"}
To lower the collision probability we concatenate $\tau$ hash functions
$$
\hat{h}(\vec{x}) = \langle h_1(\vec{x}), \dots, h_\tau(\vec{x}) \rangle
$$
this makes for a better _precision_ of the index.
:::


:::{.absolute bottom=0 left="54%" fragment-index=2}
```{python}
#plot_cp(w, [0.5,1,2])
plot_cp(w, 1, [1,2])
plt.savefig("imgs/steam-cp-kmany-l1.png", dpi=300)
```
:::

## {.smaller}

:::{.absolute top="0%" fragment-index=1}
```{python}
#| layout-ncol: 2
for i, seed in enumerate([1234, 243,256,3562,235,1]):
    plot_hashes(prj, k=2, r=0.25, seed=seed, size=2, title=f"Repetition {i+1}")
    plt.savefig(f"imgs/steam-hashes-k2-{seed}.png", dpi=300)
```
:::

:::{.absolute top=10 left="54%"}
And to increase the _recall_ of the index we repeat $L$ times.
:::


:::{.absolute bottom=0 left="54%" fragment-index=2}
```{python}
plot_success_p(w, 1, k=2, ls=[1, 10, 100], title="For τ=2")
plt.savefig("imgs/steam-sp.png", dpi=300)
```
:::

## Computing the success probability {.smaller}

::::{.columns}
:::{.column}
- Consider a distance $d$
- Assume to have done $L$ LSH repetitions with $\tau$ concatenations
- We can compute the probability of having seen at least once all pairs at distance $d$
:::

:::{.column}
```{python}
plot_success_p(w, 1, k=2, ls=[100], title="For τ=2 and L=100")
```
:::
::::

## A simple algorithm

![](algo.png)

## Auto tuning parameters 

- How do we set $\tau$?
- The wrong value might make us do too many repetitions

. . .

- We adopt an approach that auto tunes the parameters based on the data at hand 

<!-- . . . -->
<!-- - See the paper for details -->

## {.center visibility="visible"}

- $L_{max}$ maximum number of repetitions,
- $\tau_{max}$ maximum number of concatenations (e.g. 4),
- $\delta$ probability parameter: succeed with probability $\ge 1-\delta$.

::::{.columns .fragment}
:::{.column}
![Repetition 1](imgs/example-rep1.png){width="100%"}
:::

:::{.column}
![Repetition 2](imgs/example-rep3.png){width="100%"}
:::
::::

## {.smaller .center visibility="visible"}

In each iteration we compute the distance of all subsequences in the same
bucket.

::::{.r-stack}

<!---- k = 4 ------------------------>
:::{.bg-white}
```{python}
k=4
dist = 1
prevs = []
plot_execution(w, r=1, k=k, max_reps=100, rep=1, p_threshold=0.9, dist=dist + 1, prevs=prevs)
prevs.append((k, 1))
```
In the first iteration, with $k=4$, we discover a pair at distance 2<br/>
<span class="text-white">.</span>
:::
:::{.fragment .bg-white}
```{python}
plot_execution(w, r=1, k=k, max_reps=100, rep=10, p_threshold=0.9, dist=dist, prevs=prevs)
prevs.append((k, 10))
```
After 10 repetitions, we find a pair at distance 1<br/>
<span class="text-white">.</span>
:::
:::{.fragment .bg-white}
```{python}
plot_execution(w, r=1, k=k, max_reps=100, rep=100, p_threshold=0.9, dist=dist, prevs=prevs)
prevs.append((k, 100))
```
After 100 repetitions we did not find a better pair,<br/> and the success probability 
is about 0.85
:::
<!---- k = 3 ------------------------>
:::{.fragment .bg-white}
```{python}
k=3
plot_execution(w, r=1, k=k, max_reps=100, rep=1, p_threshold=0.9, dist=dist, prevs=prevs)
prevs.append((k, 1))
```
We then consider shorter prefixes of the hashes, <br/>going through the 100
repetitions again.
:::
:::{.fragment .bg-white}
```{python}
plot_execution(w, r=1, k=k, max_reps=100, rep=15, p_threshold=0.9, dist=dist, prevs=prevs)
prevs.append((k, 15))
```
After 15 repetitions, we observe that the<br/> success probability is above our
target, and thus return.
:::
::::

## Guarantees


:::{#thm-recall}
Our algorithm finds the **exact** top-$k$ motifs each with probability $\ge 1-\delta$.
:::

::: {.fragment}
- For $\delta=0.1$, each true motif is found with 90% probability
- This means that we can control the _recall_ of the algorithm
- **Tradeoff**: the higher the desired recall, the slower the discovery process
:::


## Complexity {.smaller}

:::{#thm-index}
The LSH index construction takes time
$$
O(\tau_{max} \cdot \sqrt{L_{max}}\; n\log n)
$$
:::


:::{#thm-motif}
Let $d(m_k)$ be the distance of the $k$-th motif,
and $i'\le \tau_{max}$, $j' \le L_{max}$ be the parameters used
by the optimal LSH algorithm.
Then, the algorithm considers
$$
O\left(
j'\sum_{m\in T^w\times T^w} p(d(m))^{i'}
+
(L_{max}-j')\sum_{m\in T^w\times T^w} p(d(m))^{i'+1}
\right)
$$
pairs in expectation.
:::


## Optimizations {visibility="hidden"}

- Use a trie data-structure to re-use computations across iterations at different $k$ values
- Compute dot producs for hash values in the frequency domain (also done in some implementations of the Matrix Profile)
- Compute fewer hash values using _tensoring_


## Experimental results {.smaller}

:::::{.columns}

::::{.column}
Find the top-10 motifs.

:::{style="font-size: 1.3rem;"}
| dataset | $n$  (millions) | RC |
|:--------|----:|----------------:|
| astro   | 1  |  8.63 |
| GAP     | 2  | 9.17 |
| freezer | 7  | 7.95 |
| ECG     | 7  | 109.06 |
| HumanY  | 26 | 581.03 |
| Whales  | 308 | 21.66 |
| Seismic | 1000 | 274.44 |
:::

Relative Contrast measures difficulty: higher is easier.
$$
RC = \frac{d_{avg}}{d_{motif}}
$$
::::

::::{.column .fragment}
![](imgs/10-motifs-simple.png){width="100%"}
::::

:::::

## Scalability {.smaller}

![Synthetic data, planted motif.](imgs/scalability_n_linear.png){width="100%"}

## Practical takeaways {visibility="visible"}

:::{.incremental}

- For shorter time series, or if the relative contrast is very small, use the
  Matrix Profile.

- For time series of a few million values and above, with a not-to-small relative
  contrast, try `Attimo`

- `Attimo` gives you control over the recall, and adapts to the data distribution
:::

## References {.smaller}

- Matteo Ceccarello, Johann Gamper:
_Fast and Scalable Mining of Time Series Motifs with Probabilistic Guarantees._
Proc. VLDB Endow. 15(13): 3841-3853 (2022)

::::{.columns}
:::{.column}
<center>
https://github.com/Cecca/attimo

![](imgs/qr-attimo-repo.png)
</center>
:::
:::{.column}
<br/>
<br/>
```{.python}
import pyattimo
ts = pyattimo.load_dataset("ecg", prefix=1000000)
motifs = pyattimo.MotifsIterator(
    ts, 
    w=1000
)
m = next(motifs)
```
:::
::::

<center>
  `matteo.ceccarello@unipd.it`
</center>

## Appendix {visibility="uncounted"}

## Influence of $L_{max}$ {visibility="uncounted"}

![Running for top-10 motifs, for different number of repetitions.](imgs/repetitions.png)

## Difficult datasets {.smaller visibility="uncounted"}

Data from LIGO:

```{python}
case1 = np.loadtxt("imgs/case1.txt")
plot_ts(case1[:2000], w=1000, highlight=[100])
```

::::{.columns}
:::{.column}
- Length 100k, window 1k
- Top motif distance $\approx 40$
- Average distance $\approx 44$
- Relative contrast $\approx 1.1$
:::
:::{.column}
- `Attimo`: 6 hours
- `SCAMP`: $\approx 1$ second
:::
::::

:::{.aside}
Thanks to Eamonn Keogh and Nathan Romanelli for providing and discussing this data.
:::


## `freezer` and the 7-th motif {.smaller visibility="uncounted"}

7M points, window 5000

```{python}
w = 5000
freezer = np.loadtxt("../../data/freezer.txt.gz")
motifs_mp = [
  (3705031, 1834102, 4.195242485),
  (3698075, 4733298, 5.765751866),
  (2352371, 4186995, 7.077046765),
  (4002563, 3993450, 7.318316307),
  (4618976, 4812738, 9.207241828),
  (1825969, 1993859, 9.366285725),
  (1697587, 1408089, 10.56533893),
  (5169982, 6429402, 11.46242184),
  (6641806, 5230708, 12.46052708),
  (6339277, 191377, 12.50717434),
  (36578, 3988437, 12.73866733)
]
i, j = (3815625, 5170040)
d = zeucl(freezer[i:i+w], freezer[j:j+w])

plt.figure(figsize=(14,3))
plt.axhline(0, 0, 13, c="gray")

last = 0
for ii, jj, dd in motifs_mp:
    off = 0.04 if dd - last > 0.6 else 0.08
    if dd > 12.7:
        off = 0.12      
    last = dd
    plt.plot([dd,dd], [0, off], c="gray", linewidth=0.2)
    plt.scatter(dd, 0, c="black")  
    plt.text(dd, off, f"{dd:.3f}", fontsize=14, va="top")

off = -0.06
plt.plot([d, d], [0, off], c="firebrick")
plt.scatter(d, 0, c="firebrick")  
plt.text(d, off, f"{d:.3f}", color="firebrick", fontsize=14, va="bottom")

height = 0.16
plt.gca().set_ylim((-height, height))

plt.axis("off")
plt.show()
```

- In black are the distances of the top-10 motifs extracted from the matrix
  profile.
- In red the distance of a pair of subsequences neither of which is the nearest
  neighbor of the other, and not overlapping with higher-ranked motifs.
- The matrix profile holds the distances and indices of the 1-nearest neighbor
  of each subsequence, but top-k motifs would require the k-nearest neighbors
  to be maintained in the matrix profile.

## Top-k and k-NN {.smaller visibility="uncounted"}

::::{.columns}
:::{.column}
```{python}
#| out-width: "100%"
points = np.array([
  (0,0),
  (2,0),
  (0.8, 0.3),
  (2-0.8, 0.3)
])
shapes = [
  "o", "o",
  "D", "D"
]
colors = [
  "darkorange", "darkorange",
  "blue", "blue"
]
plt.figure(figsize=(5,5))
order = [0,2,3,1]
plt.plot(points[order, 0], points[order,1], linestyle="-", c="black", zorder=0)
plt.plot(points[[0,1], 0], points[[0,1],1], linestyle="--", c="firebrick", zorder=0)

for i in range(points.shape[0]):
    plt.scatter(points[i,0], points[i,1], c=colors[i], marker=shapes[i], zorder=1)

plt.gca().set_aspect('equal')
plt.axis("off")
plt.gca().set_ylim((-0.1, 0.4))
plt.tight_layout()
plt.show()
```
:::
:::{.column}
- Solid lines are nearest neighbor distances
- The dashed line is the distance of the top-2 pair in the definition we are using
- The Matrix Profile contains information about 1-nearest neighbors (solid black lines)
:::
::::



